{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PART A.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNGKvUfO75gwUjyce4c8/8S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nkiru-ede/nkiru_codes/blob/master/PART_A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install twython"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeuLAUcnBAha",
        "outputId": "cbc6245f-b3ab-4969-9008-993eeddd8618"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: twython in /usr/local/lib/python3.7/dist-packages (3.9.1)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from twython) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from twython) (1.3.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->twython) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.4.0->twython) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "id": "mZHPHuieSdSc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53b226b7-d058-4d91-bd67-6a7726c1794b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gmplot"
      ],
      "metadata": {
        "id": "AgOZWrd2Sd-M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9d48908-c11e-45c6-ea8c-bd395861cdf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gmplot\n",
            "  Downloading gmplot-1.4.1-py3-none-any.whl (164 kB)\n",
            "\u001b[?25l\r\u001b[K     |██                              | 10 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |████                            | 20 kB 16.9 MB/s eta 0:00:01\r\u001b[K     |██████                          | 30 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |████████                        | 40 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 51 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 61 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 71 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 81 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 92 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 102 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 112 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 122 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 133 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 143 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 153 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 163 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 164 kB 5.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gmplot) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gmplot) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gmplot) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gmplot) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gmplot) (2021.10.8)\n",
            "Installing collected packages: gmplot\n",
            "Successfully installed gmplot-1.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "E1pXf9teShvN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0225d693-87c7-4838-cca3-438e623dab05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "0m9DJNcQSn68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af75a9f1-8b14-49f5-bf5c-9eb3bfb7cfb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing important libraries\n",
        "import pandas as pd\n",
        "import tweepy\n",
        "from textblob import TextBlob\n",
        "from wordcloud import WordCloud\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('fivethirtyeight')\n",
        "from datetime import date\n",
        "from datetime import time\n",
        "from datetime import datetime\n",
        "import seaborn as sns\n",
        "import matplotlib.font_manager\n",
        "from itertools import cycle, islice\n",
        "import numpy as np\n",
        "import tweepy\n",
        "import requests\n",
        "import base64\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import corpus\n",
        "from geopy.geocoders import Nominatim\n",
        "import numpy as np\n",
        "import gmplot\n",
        "import webbrowser\n",
        "from twython import Twython\n",
        "import json\n",
        "from twython import Twython\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud"
      ],
      "metadata": {
        "id": "QyDrzzDGR2mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get Twitter credentials into Json format\n",
        "credentials={}\n",
        "credentials['CONSUMER_KEY'] = 'k45uvcP2VAYtiMcv5WPZmf6wC'\n",
        "credentials['CONSUMER_SECRET'] = 'g2KxMpvOBj5ebQpExDVFAuYHrkTU99ICAcdNBprbTHieS6US0n'\n",
        "credentials['ACCESS_TOKEN'] = '3102441035-Wdz2E2JPzu9Piom64ZAQ9rJZ3JbOYNkEHJAwM2C'\n",
        "credentials['ACCESS_SECRET'] = 'GyFKlqD6hgfjZGsN6j3YMq3peEcPw96I0FyrM5Mfvqn1y'\n",
        "\n",
        "with open(\"twitter_credentials.json\", \"w\") as file:\n",
        "    json.dump(credentials, file)"
      ],
      "metadata": {
        "id": "Yzm4V5MuSHtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#What are the popular trends on Twitter at the moment?\n",
        "#Get Trends by Location - Uk (29/04/2022)\n",
        "#Get Twitter credentials   \n",
        "with open(\"twitter_credentials.json\", \"r\") as file:\n",
        "    creds = json.load(file)  \n",
        "python_tweets = Twython(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'])\n",
        "#get trends in the UK with the WOEID\n",
        "PlaceTrends = python_tweets.get_place_trends(id = 23424975)\n",
        "#Extract tweet data from API\n",
        "dict_ = {'query': [], 'Hashtag': [], 'tweet_volume': [], 'promoted_content' : [],'url': [] }\n",
        "for status in PlaceTrends[0] ['trends']:\n",
        "   dict_['query'].append(status['query'])\n",
        "   dict_['Hashtag'].append(status['name'])\n",
        "   dict_['tweet_volume'].append(status['tweet_volume'])\n",
        "   dict_['promoted_content'].append(status['promoted_content'])\n",
        "   dict_['url'].append(status['url'])\n",
        "   df = pd.DataFrame(dict_)\n",
        "df.sort_values(by='tweet_volume', inplace = True, ascending =False)\n",
        "#print new DataFrame, df\n",
        "df"
      ],
      "metadata": {
        "id": "RSyWtSPXSTLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sort tweet volume and hashtag only\n",
        "df.sort_values(by='tweet_volume', inplace = True, ascending =False)\n",
        "#print new DataFrame, df\n",
        "df"
      ],
      "metadata": {
        "id": "oIcgQi3fTad0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cleaning the data - removing '#'\n",
        "def cleanTxtH(cTx):\n",
        "  cTx = re.sub(r'#', '', cTx) #remove the '#' symbol\n",
        "  return cTx\n",
        "#cleaning the text\n",
        "df['Hashtag'] = df['Hashtag'].apply(cleanTxtH)\n",
        "#Remove records with hashtag = NaN\n",
        "df = df.dropna()\n",
        "#show the cleaned text\n",
        "df"
      ],
      "metadata": {
        "id": "yzNEs8oGT16F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualise popular trends in the UK using the WordCloud\n",
        "allWords = ' '.join([twts for twts in df['Hashtag']])\n",
        "wordCloud = WordCloud(width = 500, height = 300, background_color= 'white' ,random_state = 21, max_font_size = 209).generate(allWords)\n",
        "plt.imshow(wordCloud, interpolation = \"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hBftgPiZT99r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualise the popular trends in the United Kingdom using a Barchart\n",
        "my_colors = list(islice((['b', 'r', 'g', 'y', 'k']), len(df)))\n",
        "barC = df.plot(kind = 'bar', x='Hashtag', y='tweet_volume',rot=90, figsize=(16, 8),stacked=True, color = my_colors, legend=False)\n",
        "barC.set_title(\"Popular Trends in the UK\")\n",
        "barC.set_ylabel(\"Tweet Volume\")"
      ],
      "metadata": {
        "id": "2KXOUGGDUHMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Determine when trend started  - using favourite count\n",
        "#Load twitter credentials\n",
        "with open(\"twitter_credentials.json\", \"r\") as file:\n",
        "  creds=json.load(file)\n",
        "python_tweets = Twython(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'])\n",
        "#Query topics that trended in the last 8 days\n",
        "TrendDates =[\"2022-05-01\",\"2022-04-30\",\"2022-04-29\",\"2022-04-28\",\"2022-04-27\",\"2022-04-26\",\"2022-04-25\",\"2022-04-24\",\"2022-04-23\",\"2022-04-22\"]\n",
        "tweets = []\n",
        "for i in TrendDates:\n",
        "  query = {'q' : 'Neil Parish',\n",
        "  'result_type' : 'mixed',\n",
        "  'count' : 100, 'lang' : 'en', \"until\":i}\n",
        "  sample_return = python_tweets.search(**query)\n",
        "  tweets.append(sample_return['statuses'])\n",
        "  #tweets = tweets + sample_return['statuses']\n",
        "  print(len(sample_return['statuses']))\n",
        "dfs =[]\n",
        "for i in tweets:\n",
        "  dfs.append(pd.DataFrame(i))\n",
        "  totals=[]\n",
        "for i in dfs:\n",
        "  totals.append(i[\"favorite_count\"].sum())\n",
        "#Plot Trend dates against Favorite count\n",
        "plt = sns.barplot(y=TrendDates, x=totals).set(title=\"Neil Parish(Trend Dates/Favorite Count)\")"
      ],
      "metadata": {
        "id": "3AZ11bIhUQN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Determine when trend started - using Retweet count\n",
        "#Load Twitter credentials\n",
        "with open(\"twitter_credentials.json\", \"r\") as file:\n",
        "  creds=json.load(file)\n",
        "  python_tweets = Twython(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'])\n",
        "  #Query topics that trended in the last 8 days\n",
        "  TrendDates =[\"2022-04-29\",\"2022-04-28\",\"2022-04-27\",\"2022-04-26\",\"2022-04-25\",\"2022-04-24\",\"2022-04-23\",\"2022-04-22\"]\n",
        "  tweets = []\n",
        "for i in TrendDates:\n",
        "  query = {'q' : 'Neil Parish',\n",
        "  'result_type' : 'mixed',\n",
        "  'count' : 100, 'lang' : 'en', \"until\":i}\n",
        "  sample_return = python_tweets.search(**query)\n",
        "  tweets.append(sample_return['statuses'])\n",
        "  print(len(sample_return['statuses']))\n",
        "dfs =[]\n",
        "for i in tweets:\n",
        "  dfs.append(pd.DataFrame(i))\n",
        "  totals=[]\n",
        "for i in dfs:\n",
        "  totals.append(i['retweet_count'].sum())\n",
        "#Plot Trend dates against Retweet count\n",
        "sns.barplot(y=TrendDates, x=totals).set(title=\"Neil Parish(Trend Dates/Retweet Count)\")"
      ],
      "metadata": {
        "id": "wTQ8dvVNU4-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#when trend started in each place - using retweet count and favourite count\n",
        "#Query topics that trended in the last 8 days\n",
        "dates =[\"2022-04-29\",\"2022-04-29\",\"2022-04-28\",\"2022-04-27\",\"2022-04-26\",\"2022-04-25\",\"2022-04-24\",\"2022-04-23\"]\n",
        "response = []\n",
        "for i in dates:\n",
        "  query = {'q' : 'neil parish', 'result_type' : 'mixed', 'count' : 100, 'lang' : 'en', \"until\":i}\n",
        "  sample_return = python_tweets.search(**query)\n",
        "  response.append(sample_return['statuses'])\n",
        "  print(len(sample_return['statuses']))\n",
        "#convert Dictionary to DataFrame\n",
        "dfs =[]\n",
        "for i in response:\n",
        "  dfs.append(pd.DataFrame(i))\n",
        "#get retweet and favorite count\n",
        "favorite=[]\n",
        "retweet=[]\n",
        "for i in dfs:\n",
        "  retweet.append(i[\"retweet_count\"].sum())\n",
        "  favorite.append(i[\"favorite_count\"].sum())\n",
        "datess = dates + dates\n",
        "values = favorite + retweet\n",
        "label = []\n",
        "for i in range (8):\n",
        "  label.append(\"Favorite\")\n",
        "for i in range (8):\n",
        "  label.append(\"Retweet\")\n",
        "toPlot = list(zip(datess,values, label))\n",
        "#Plot Trend dates against Retweet and Favorite counts\n",
        "toPlot_final = pd.DataFrame(toPlot, columns = ['Tweet_Dates', 'Favorite&Retweet_Count', 'Type'])\n",
        "sns.barplot(x=\"Favorite&Retweet_Count\", y = \"Tweet_Dates\", hue = \"Type\", data = toPlot_final).set(title=\"Retweet and Favorite Count\")"
      ],
      "metadata": {
        "id": "KYx6Ou-EVYIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#What devices were used to tweet?\n",
        "#Spool twitter parameter 'Source' \n",
        "python_tweets = Twython(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'])\n",
        "query = {'q': 'Neil Parish',\n",
        "         'result_type': 'mixed',\n",
        "         'count': 100, 'lang': 'en', 'until': '2022-04-29'}\n",
        "sample_return = python_tweets.search(**query)\n",
        "dict_={'user': [], 'date':[], 'text': [], 'source': [],'user': [],'Verified_status':[],'followers_count':[]}\n",
        "for status in python_tweets.search(**query) ['statuses']:\n",
        "    dict_['user'].append(status['user']['screen_name'])\n",
        "    dict_['date'].append(status['created_at'])\n",
        "    dict_['text'].append(status['text'])\n",
        "    dict_['source'].append(status['source']\n",
        "    dict_['user'].append(status['user']['screen_name'])\n",
        "    dict_['Verified_status'].append(status['user']['verified'])\n",
        "    dict_['followers_count'].append(status['user']['followers_count'])\n",
        "df = pd.DataFrame(dict_)\n",
        "#cleaning the data \n",
        "def cleanTxt(cTx):\n",
        "  cTx = re.sub(r'<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>', 'iphone', cTx) #Remove @mentions\n",
        "  cTx = re.sub(r'<a href=\"http://twitter.com/#!/download/ipad\" rel=\"nofollow\">Twitter for iPad</a>', 'iPad', cTx)\n",
        "  cTx = re.sub(r'<a href=\"https://mobile.twitter.com\" rel=\"nofollow\">Twitter Web App</a>', 'Twitter Web App', cTx)\n",
        "  cTx = re.sub(r'<a href=\"https://about.twitter.com/products/tweetdeck\" rel=\"nofollow\">TweetDeck</a>', 'TweetDeck', cTx)\n",
        "  cTx = re.sub(r'<a href=\"https://studio.twitter.com\" rel=\"nofollow\">Twitter Media Studio</a>', 'Twitter Media Studio', cTx)\n",
        "  cTx = re.sub(r'<a href=\"http://twitter.com/download/android\" rel=\"nofollow\">Twitter for Android</a>', 'Andriod', cTx)\n",
        "  cTx = re.sub(r'<a href=\"http://publicize.wp.com/\" rel=\"nofollow\">WordPress.com</a>', 'WordPress', cTx)\n",
        "  #cTx = re.sub(r'<a href=\"https://studio.twitter.com\" rel=\"nofollow\">Twitter Media Studio</a>', 'Twitter Media Studio', cTx)\n",
        "  #cTx = re.sub(r'<a href=\"https://studio.twitter.com\" rel=\"nofollow\">Twitter Media Studio</a>', 'Twitter Media Studio', cTx)\n",
        "  return cTx\n",
        "#cleaning the text\n",
        "df['source'] = df['source'].apply(cleanTxt)\n",
        "#show the cleaned text\n",
        "df\n",
        "#create piechart/plot used devices\n",
        "plt.pie(y, labels = mylabels,shadow = True,autopct='%.0f%%')\n",
        "plt.legend(title = \"Devices used to tweet:\",bbox_to_anchor=(1,1), loc=\"upper left\")\n",
        "plt.show() "
      ],
      "metadata": {
        "id": "gMsnBhMeVxCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sources we can trust - verified status\n",
        "#sort dataframe by Verified status\n",
        "df.sort_values(by='Verified_status', inplace = True, ascending =False)\n",
        "df "
      ],
      "metadata": {
        "id": "dNQ5JxT1WR17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sort dataframe by followers count\n",
        "df.sort_values(by='followers_count', inplace = True, ascending =False)\n",
        "#print sources to trust -followers count - word cloud, barchart\n",
        "allWords = ' '.join([twts for twts in df['user']])\n",
        "wordCloud = WordCloud(width = 500, height = 300, background_color= 'white' ,random_state = 21, max_font_size = 209).generate(allWords)\n",
        "plt.imshow(wordCloud, interpolation = \"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "my_colors = list((['b', 'r', 'g', 'y', 'k']))\n",
        "barC = df.plot(kind = 'bar', x='user', y='followers_count',rot=90, figsize=(16, 8),stacked=True, color = my_colors, legend=False)\n",
        "barC.set_title(\"Users Followers Count\")\n",
        "barC.set_ylabel(\"Number of Followers\")"
      ],
      "metadata": {
        "id": "w69W2phDWwJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#HeatMap/Location of the trend\n",
        "Location = df['location']\n",
        "Location = Location.dropna()\n",
        "\n",
        "    \n",
        "def remove_emoji(string):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               u\"\\U0001f926-\\U0001f937\"\n",
        "                               u\"\\U00010000-\\U0010ffff\"\n",
        "                               u\"\\u2640-\\u2642\"\n",
        "                               u\"\\u2600-\\u2B55\"\n",
        "                               u\"\\u200d\"\n",
        "                               u\"\\u23cf\"\n",
        "                               u\"\\u23e9\"\n",
        "                               u\"\\u231a\"\n",
        "                               u\"\\ufe0f\"  # dingbats\n",
        "                               u\"\\u3030\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', string)\n",
        "\n",
        "Location_clean = []\n",
        "for i in Location:\n",
        "    Location_clean.append(remove_emoji(i))\n",
        "\n",
        "geolocator = Nominatim(user_agent=\"geo\")\n",
        "def geolocate(country):\n",
        "    try:\n",
        "        # Geolocate the center of the country\n",
        "        loc = geolocator.geocode(country)\n",
        "        # And return latitude and longitude\n",
        "        return (loc.latitude, loc.longitude)\n",
        "    except:\n",
        "        # Return missing value\n",
        "        return np.nan\n",
        "    \n",
        "Location_test = []\n",
        "for i in Location_clean:\n",
        "    Location_test.append(geolocate(i))\n"
      ],
      "metadata": {
        "id": "Fw6G1P23XwdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove invalid rows\n",
        "Location_test = [x for x in Location_test if str(x) != 'nan']\n",
        "#add headers to new dataframe\n",
        "Location_test = pd.DataFrame(Location_test, columns=[\"Latitude\", 'Longitude'])\n",
        "Location_test = Location_test.dropna()\n",
        "Lat = Location_test['Latitude']\n",
        "Long = Location_test['Longitude']\n",
        "map_plot = gmplot.GoogleMapPlotter(53.81604806664296, -3.0548307614209813, 3)\n",
        "map_plot.heatmap(Lat, Long)\n",
        "#draw map and save html file\n",
        "map_plot.draw(\"test.html\")\n",
        "#open new webbrower to display map\n",
        "webbrowser.open_new_tab(\"test.html\")"
      ],
      "metadata": {
        "id": "OYh6MmoeYA0y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}